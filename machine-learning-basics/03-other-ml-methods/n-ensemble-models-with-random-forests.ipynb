{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd07e7ed898d369334e998bf643179707e62ca7f11d6a1140b26f2ba8bce3c4feca",
   "display_name": "Python 3.8.3 64-bit ('env-ml': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "7e7ed898d369334e998bf643179707e62ca7f11d6a1140b26f2ba8bce3c4feca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Ensemble models with random forests"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Ensemble methods are machine learning methods that combine several based models to produce one optimal predictive model.  \n",
    "They combine decisions from multiple models to improve the overall performance.  \n",
    "Ensemble learning involves creating a collection, or ensemble, of multiple algorithms for the purpose of generating a single model that's far more powerful and reliable than its component parts.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Types of ensemble methods\n",
    "\n",
    "- majority voting\n",
    "- averaging\n",
    "- weighted averaging\n",
    "- bagging\n",
    "- boosting\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## The majority voting method \n",
    " \n",
    "Picks the result based on the majority of votes from different models.  \n",
    "This method is generally used in classification problems.  \n",
    "\n",
    "## The averaging method \n",
    "\n",
    "Is quite similar to majority voting. Multiple models are run, and predictions are averaged.  \n",
    "Averaging method can be used in both classification and regression problems.\n",
    "\n",
    "## The weighted average method\n",
    "\n",
    "Uses multiple models to make predictions. The method allocates weights to different model predictions, and averages them out.\n",
    "\n",
    "## Bagging\n",
    "\n",
    "Is a method wherein the results from multiple models are combined to get a final result.  \n",
    "Decision trees are used frequently with bagging.  \n",
    "The main idea of bagging is to create subsets of the original data and run different models on the subsets.  \n",
    "Finally, the results are aggregated. Bagging works in parallel.  \n",
    "\n",
    "## Boosting\n",
    "\n",
    "Is a slightly more complex version of bagging.  \n",
    "Boosting has a sequential approach. The six main steps of boosting are:\n",
    "\n",
    "- create a subset of the data\n",
    "- run a model on the subset of the data and get the predictions\n",
    "- calculate errors on those predictions\n",
    "- assign weight to the incorrect predictions\n",
    "- create another model with the same data, and the next subset of data is created\n",
    "- the cycle repeats itself until a strong learner is created\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Random forest\n",
    "\n",
    "Is an ensemble model which follows the bagging method.  \n",
    "This model uses decision trees to form ensembles.  \n",
    "This approach is useful for both classification and regression problems.  \n",
    "\n",
    "### How random forests works\n",
    "\n",
    "When predicting a new value for a target feature, each tree is either using regression or classification to come up with a value that serves as a vote.  \n",
    "The random forest algorithm then takes an average of all votes from all trees in the ensemble.  \n",
    "This average is the predicted value of the target feature for the variable in question.\n",
    "\n",
    "### There are five main steps in random forest\n",
    "\n",
    "- createa random subset from the original data\n",
    "- randomly select a feature at each node in the decision tree\n",
    "- the best split is decided\n",
    "- for each subset of data, a separate model is created (this is called a *base learner*)\n",
    "- compute the final prediction by averaging the predictions from all the individual models\n",
    "\n",
    "## The advantages of random forest are\n",
    "\n",
    "- Easy to understand\n",
    "- Useful for data exploration\n",
    "- Reduced data cleaning (scaling not required)\n",
    "- Highly flexible\n",
    "- Gives good accuracy.\n",
    "- Works well on large datasets\n",
    "- Handle multiple data types\n",
    "- Overfitting is avoided (due to averaging)\n",
    "\n",
    "## The disadvantages of random forest are\n",
    "\n",
    "- Does not work well with sparse datasets\n",
    "- Requires a bit of computational resources to run\n",
    "- No interpretability\n",
    "- Not for continuous variables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}