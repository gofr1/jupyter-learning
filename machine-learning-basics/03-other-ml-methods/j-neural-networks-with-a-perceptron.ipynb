{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd07e7ed898d369334e998bf643179707e62ca7f11d6a1140b26f2ba8bce3c4feca",
   "display_name": "Python 3.8.3 64-bit ('env-ml': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "7e7ed898d369334e998bf643179707e62ca7f11d6a1140b26f2ba8bce3c4feca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Neural Networks with Perceptron"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Fundamental concepts\n",
    "\n",
    "An input layer -> Weights and bias -> A weighted sum -> An activation function -> Linear separability \n",
    "\n",
    "## Perceptron \n",
    "\n",
    "A perceptron is a neural network with just one layer. It's a linear classifier that outputs a binary response variable. Consequently, the algorithm is called \"linear binary classifier\".\n",
    "\n",
    "## Linear Separability\n",
    "\n",
    "- Data is said to have \"linear separability\" if it can be cleanly classified into one of two classes. \n",
    "- Your data must be linearly separable in order for perceptron to operate properly "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Four essential elements of the perceptron\n",
    "\n",
    "1. An input layer\n",
    "2. Weights and bias \n",
    "3. A weighted sum \n",
    "4. An activation function\n",
    "\n",
    "## Single-Layer Neutral Network\n",
    "\n",
    "|Input layer | | Weighted Sum | Activation function | |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "|Input Feature 1 | Weight 1 |                     | Simple   |\n",
    "|Input Feature 2 | Weight 2 | Weighted Sum / bias | Linear   | Output layer (y^)\n",
    "|Input Feature 3 | Weight 3 |                     | Function |"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Activation Function\n",
    "\n",
    "An activation function is a mathemetical function that is deployed on each unit in a neural network. All units in the shared layer deploy the same activation function. The purpose of activation functions is to enable neural networks to model complex, nonlinear phenomenon.\n",
    "\n",
    "- Linear activation: tf.matmul() - single layer perceptron\n",
    "- Logistic sigmoid: use these often in the final output layer, useful with binary input features\n",
    "- Threshold function: useful with binary features\n",
    "- ReLU (rectified linear unit)\n",
    "- SoftMax"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import sklearn\n",
    "\n",
    "from pandas import Series, DataFrame\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "x = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "standardize = StandardScaler()\n",
    "\n",
    "standardize_x_test = standardize.fit_transform(x_test)\n",
    "standardize_x_train = standardize.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-0.08196892, -0.76255128,  0.27241258,  0.22358746],\n",
       "       [ 0.43572953, -0.76255128,  0.61530954,  0.09206542],\n",
       "       [ 1.21227721,  0.41665173,  1.18680446,  1.27576373],\n",
       "       [-0.8585166 ,  1.59585474, -1.15632473, -1.22315491],\n",
       "       [ 0.82400337,  0.18081113,  0.50101055,  0.48663152]])"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "standardize_x_test[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Perceptron(eta0=0.15, max_iter=50, random_state=15)"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "# Create a perceptron\n",
    "perceptron = Perceptron(max_iter=50, eta0=.15, tol=1e-3, random_state=15)\n",
    "## train model\n",
    "perceptron.fit(standardize_x_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Make a prediction using created model\n",
    "y_pred = perceptron.predict(standardize_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1 1 2 0 1 2 0 1 1 2 0 0 0 2 0 1 1 2 0 2 1 0 1 2 1 2 0 0 2 0]\n[1 1 2 0 2 2 0 2 1 2 0 1 1 2 0 1 1 2 1 1 2 0 2 2 1 2 1 0 2 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           0       1.00      0.55      0.71        11\n           1       0.50      0.60      0.55        10\n           2       0.67      0.89      0.76         9\n\n    accuracy                           0.67        30\n   macro avg       0.72      0.68      0.67        30\nweighted avg       0.73      0.67      0.67        30\n\n"
     ]
    }
   ],
   "source": [
    "# Check the accuracy of the prediction\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "source": [
    "**Precision** is a measure of the model's relevancy  \n",
    "**Recall** is a measure of model's completeness  \n",
    "\n",
    "In our case for precision:\n",
    "\n",
    "0: For all the points predicted to have 0 label, 100% of the retrieved instances were relevant  \n",
    "1: For all the points predicted to have 1 label, 50% of the retrieved instances were relevant  \n",
    "\n",
    "For recall:\n",
    "\n",
    "2: For all your points that were labeled 2, 89% of the results that were returned were trully relevant  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}