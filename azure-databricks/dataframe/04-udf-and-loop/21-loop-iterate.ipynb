{"cells":[{"cell_type":"markdown","source":["## PySpark Loop/Iterate Through Rows in DataFrame\n\nPySpark provides `map()` and `mapPartitions()` to loop/iterate through rows in RDD/DataFrame to perform the complex transformations, and these two returns the same number of records as in the original DataFrame but the number of columns could be different (after add/update).\n\nPySpark also provides `foreach()` and `foreachPartitions()` actions to loop/iterate through each Row in a DataFrame but these two returns nothing.\n\nIteration could be done using:\n* map(), \n* foreach(), \n* converting to Pandas, \n* converting DataFrame to Python List"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b5d9fc83-b223-4022-add7-37433a769f59"}}},{"cell_type":"code","source":["dbutils.library.restartPython() # Removes Python state, but some libraries might not work without calling this command.dbutils.restartPython()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f4dac6b-ee25-4716-8e80-d1257d300e56"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Load libraries"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a280b9b-00d3-45a7-ab54-f88a7376e037"}}},{"cell_type":"code","source":["from pyspark.sql import SparkSession, Row\nfrom pyspark.sql.types import IntegerType, DateType, StringType, StructType, StructField, ArrayType, MapType, DoubleType\nfrom pyspark.sql.functions import lit, col, expr, when, sum, avg, max, min, mean, count, udf, explode, concat_ws"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e0d5fac-1ce1-4733-9bc0-4f9cc891d982"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Create Spark session"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dbed0d19-71c5-4c1b-8340-d6c6ca5d9083"}}},{"cell_type":"code","source":["spark = SparkSession.builder.appName('PySpark Loop/Iterate Through Rows in DataFrame').getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"654acd8e-adfb-4f22-94dd-d826125ee9ad"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Create a DataFrame"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"de77f996-0393-4722-95d6-eeaf1edefb04"}}},{"cell_type":"code","source":["data = [\n  ('John', 'Smith', 'M', 2500.0),\n  ('Jane', 'Doe', 'F', 500.0),\n  ('Richard', 'Marquette', 'M', 1500.0),\n  ('Israel', 'Israeli', 'M', 3000.0),\n  ('Edward', 'III', 'M', 5000.0)\n]\n \nschema = StructType([\n  StructField('firstname', StringType(),True),\n  StructField('lastname', StringType(),True),\n  StructField('gender', StringType(), True),\n  StructField('salary', DoubleType(), True)\n])\n\ncolumns = schema.fieldNames()\n\ndf = spark.createDataFrame(data=data, schema=schema)\ndf.printSchema()\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e6112c8-87a7-4e14-b096-c61d4e48b80c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"firstname","nullable":true,"type":"string"},{"metadata":{},"name":"lastname","nullable":true,"type":"string"},{"metadata":{},"name":"gender","nullable":true,"type":"string"},{"metadata":{},"name":"salary","nullable":true,"type":"double"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\">root\n |-- firstname: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: double (nullable = true)\n\n+---------+---------+------+------+\n|firstname| lastname|gender|salary|\n+---------+---------+------+------+\n|     John|    Smith|     M|2500.0|\n|     Jane|      Doe|     F| 500.0|\n|  Richard|Marquette|     M|1500.0|\n|   Israel|  Israeli|     M|3000.0|\n|   Edward|      III|     M|5000.0|\n+---------+---------+------+------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- firstname: string (nullable = true)\n-- lastname: string (nullable = true)\n-- gender: string (nullable = true)\n-- salary: double (nullable = true)\n\n+---------+---------+------+------+\nfirstname| lastname|gender|salary|\n+---------+---------+------+------+\n     John|    Smith|     M|2500.0|\n     Jane|      Doe|     F| 500.0|\n  Richard|Marquette|     M|1500.0|\n   Israel|  Israeli|     M|3000.0|\n   Edward|      III|     M|5000.0|\n+---------+---------+------+------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Loop Through Rows in DataFrame \n\nMostly for simple computations, instead of iterating through using `map()` and `foreach()`, you should use either DataFrame's `select()` or `withColumn()` in conjunction with PySpark SQL functions."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2bb72582-8e32-40ff-883a-81087fd1d0fc"}}},{"cell_type":"code","source":["df.select(\n  concat_ws(',',df.firstname,df.lastname).alias('name'),\n  df.gender,\n  lit(df.salary*1.20).alias('new_salary')\n).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"25dec94e-69ba-432e-86c2-6d2fd3660306"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-----------------+------+----------+\n|             name|gender|new_salary|\n+-----------------+------+----------+\n|       John,Smith|     M|    3000.0|\n|         Jane,Doe|     F|     600.0|\n|Richard,Marquette|     M|    1800.0|\n|   Israel,Israeli|     M|    3600.0|\n|       Edward,III|     M|    6000.0|\n+-----------------+------+----------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+------+----------+\n             name|gender|new_salary|\n+-----------------+------+----------+\n       John,Smith|     M|    3000.0|\n         Jane,Doe|     F|     600.0|\nRichard,Marquette|     M|    1800.0|\n   Israel,Israeli|     M|    3600.0|\n       Edward,III|     M|    6000.0|\n+-----------------+------+----------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Using map() to Loop\n\nPySpark doesn’t have a `map()` in DataFrame instead it’s in RDD hence you need to convert DataFrame to RDD first and then use the `map()`.  \nIt returns an RDD and you should convert RDD to PySpark DataFrame if needed."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"32b06981-5139-4c13-bdc2-e9ba29282468"}}},{"cell_type":"code","source":["rdd = df.rdd.map(lambda x: (x[0]+\",\"+x[1],x[2],x[3]*1.20))\n#rdd2 = df.rdd.map(lambda x: (x.firstname+\",\"+x.lastname,x.gender,x.salary*2))\n#rdd2 = df.rdd.map(lambda x: (x[\"firstname\"]+\",\"+x[\"lastname\"],x[\"gender\"],x[\"salary\"]*2))\n\n#def func1(x):\n#  firstName=x.firstname\n#  lastName=x.lastName\n#  name=firstName+\",\"+lastName\n#  gender=x.gender.lower()\n#  salary=x.salary*2\n#  return (name,gender,salary)\n#rdd2 = df.rdd.map(lambda x: func1(x))\n\ndf2 = rdd.toDF(['name','gender','new_salary'])\ndf2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f4f2b5db-5791-410a-8679-8e7d5e4b003c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"df2","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"name","nullable":true,"type":"string"},{"metadata":{},"name":"gender","nullable":true,"type":"string"},{"metadata":{},"name":"new_salary","nullable":true,"type":"double"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\">+-----------------+------+----------+\n|             name|gender|new_salary|\n+-----------------+------+----------+\n|       John,Smith|     M|    3000.0|\n|         Jane,Doe|     F|     600.0|\n|Richard,Marquette|     M|    1800.0|\n|   Israel,Israeli|     M|    3600.0|\n|       Edward,III|     M|    6000.0|\n+-----------------+------+----------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+------+----------+\n             name|gender|new_salary|\n+-----------------+------+----------+\n       John,Smith|     M|    3000.0|\n         Jane,Doe|     F|     600.0|\nRichard,Marquette|     M|    1800.0|\n   Israel,Israeli|     M|    3600.0|\n       Edward,III|     M|    6000.0|\n+-----------------+------+----------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Using foreach() to Loop\n\n`foreach()` is an action and it returns nothing."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"88395be7-14db-4c8f-a6be-c85a18708f3a"}}},{"cell_type":"code","source":["def f(x): print(x)\ndf.foreach(f)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cd88533b-cdb6-455e-b4d6-4b1d86111c19"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["df.foreach(lambda x: print(\"Data ==>\"+x[\"firstname\"]+\",\"+x[\"lastname\"]+\",\"+x[\"gender\"]+\",\"+str(x[\"salary\"]*2)))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6804d616-3d38-4bdc-beec-26926863e64b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Using pandas() to Iterate"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ebede7b-52dd-4ed5-baa8-83858e3bdff4"}}},{"cell_type":"code","source":["import pandas as pd\n# Use spark.sql.execution.arrow.enabled config to enable Apache Arrow with Spark\n# Apache Spark uses Apache Arrow which is an in-memory columnar format to transfer the data between Python and JVM.\nspark.conf.set('spark.sql.execution.arrow.enabled', 'true')\n\npandasDF = df.toPandas()\n\nfor index, row in pandasDF.iterrows(): print(row['firstname'], row['gender'])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"617a0d51-de2e-4761-99b8-5625ad669b29"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">John M\nJane F\nRichard M\nIsrael M\nEdward M\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">John M\nJane F\nRichard M\nIsrael M\nEdward M\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Collect Data As List and Loop Through"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bf766bf2-1c0a-47c1-b118-689a978af1b0"}}},{"cell_type":"code","source":["# Collect the data to Python List\ndataCollect = df.collect()\nfor row in dataCollect: print(f\"{row['firstname']},{row['lastname']}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e2b1c6f-6857-4185-9d2f-b7d613842b02"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">John,Smith\nJane,Doe\nRichard,Marquette\nIsrael,Israeli\nEdward,III\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">John,Smith\nJane,Doe\nRichard,Marquette\nIsrael,Israeli\nEdward,III\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Using toLocalIterator()\ndataCollect = df.rdd.toLocalIterator()\nfor row in dataCollect: print(f\"{row['firstname']},{row['lastname']}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"983d9c70-c7c6-45ab-b044-e12ccaa049a4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">John,Smith\nJane,Doe\nRichard,Marquette\nIsrael,Israeli\nEdward,III\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">John,Smith\nJane,Doe\nRichard,Marquette\nIsrael,Israeli\nEdward,III\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### The end of the notebook"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6812f029-60e8-4912-9339-cdbcc67079df"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"20-dataframe-loop-iterate","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3481582878091117}},"nbformat":4,"nbformat_minor":0}
