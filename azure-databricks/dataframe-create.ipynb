{"cells":[{"cell_type":"markdown","source":["## Create DataFrame"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a7e010af-b802-443b-ab73-2ef4907278cc"}}},{"cell_type":"code","source":["dbutils.library.restartPython() # Removes Python state, but some libraries might not work without calling this command.dbutils.restartPython()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f8d1d0b9-42cd-4661-ade7-44f021771e90"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Load libraries"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96d31990-96ad-493a-b9c8-9890feeca142"}}},{"cell_type":"code","source":["from pyspark.sql import SparkSession, Row\nfrom pyspark.sql.types import IntegerType, DateType, StringType, StructType, StructField"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2d8b8289-c957-4b9a-a7cf-f3fbcfc364c0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Create Spark session"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"871fafb4-b0c7-4ddd-920b-a66e494db1f7"}}},{"cell_type":"code","source":["spark = SparkSession.builder.appName('Create dataframe').getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b5f60b79-8ffb-481c-bd4c-c0da570dc6ed"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Prepare data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"581217eb-d879-49a2-9206-fd9e1fb16e09"}}},{"cell_type":"code","source":["columns = ['Programming Language', 'Ratings']\ndata = [('C', 12.54),\n('Python', 11.84),\n('Java', 11.54),\n('C++', 7.36),\n('C#', 4.33),\n('Visual Basic', 4.01),\n('JavaScript', 2.33),\n('PHP', 2.21),\n('Assembly language', 2.05),\n('SQL', 1.88)]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3cb394db-5134-4cb6-9fe3-d294979175d4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Create DataFrame from RDD"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"45e37941-c241-4ee9-b75f-45fedaf6defc"}}},{"cell_type":"code","source":["rdd = spark.sparkContext.parallelize(data)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b6b0e0cc-b8a0-40cc-8f44-1b442ed351a5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Using toDF() function"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2446c0a6-8bb3-476b-909e-5c03ccf31e57"}}},{"cell_type":"code","source":["# RDD doesnâ€™t have columns, the DataFrame is created with default column names\ndfFromRDD = rdd.toDF()\ndfFromRDD.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"55633214-7686-42bd-858f-7d6918421e9b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- _1: string (nullable = true)\n |-- _2: double (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- _1: string (nullable = true)\n-- _2: double (nullable = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# With columns\ndfFromRDDc = rdd.toDF(columns)\ndfFromRDDc.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a3584b5-46d3-4f3b-a977-466928a7c17f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- Programming Language: string (nullable = true)\n |-- Ratings: double (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- Programming Language: string (nullable = true)\n-- Ratings: double (nullable = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Using createDataFrame() from SparkSession"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cbc22fc2-5a58-4f67-83ec-18c7b182de61"}}},{"cell_type":"code","source":["dfFromRDDs = spark.createDataFrame(rdd).toDF(*columns)\ndfFromRDDs.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7ffe5ab0-794e-44d9-98f9-8b30c96f1b92"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- Programming Language: string (nullable = true)\n |-- Ratings: double (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- Programming Language: string (nullable = true)\n-- Ratings: double (nullable = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Create DataFrame from List Collection"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2225423c-17e1-4f89-ab21-4ad4e5ab99e9"}}},{"cell_type":"code","source":["# Using createDataFrame() from SparkSession\ndfFromDataList = spark.createDataFrame(data).toDF(*columns)\ndfFromDataList.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20c8309a-c607-43ee-815f-a4a503da3a91"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- Programming Language: string (nullable = true)\n |-- Ratings: double (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- Programming Language: string (nullable = true)\n-- Ratings: double (nullable = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Using createDataFrame() with the Row type\n\nrowData = map(lambda x: Row(*x), data) \ndfFromDataRow = spark.createDataFrame(rowData,columns)\ndfFromDataRow.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24176816-91e9-4637-b282-5b4bf53fb8de"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- Programming Language: string (nullable = true)\n |-- Ratings: double (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- Programming Language: string (nullable = true)\n-- Ratings: double (nullable = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Create DataFrame with schema"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dd475e48-6cba-4671-bfae-e9dc72c0d8e1"}}},{"cell_type":"code","source":["# Populate some data\ndata2 = [\n  ('John', '', 'Smith', '36636', 'M', 2500),\n  ('Jane', '', 'Doe', '42114', 'F', 500),\n  ('Richard', 'Laurence', 'Marquette', 97086, 'M', 1500),\n  ('Israel', '', 'Israeli', '', 'M', 3000),\n  ('Edward', 'III', '', 'SL4', 'M', 5000)\n]\n \nschema = StructType([\n  StructField(\"firstname\",StringType(),True),\n  StructField(\"middlename\",StringType(),True),\n  StructField(\"lastname\",StringType(),True),\n  StructField(\"id\", StringType(), True),\n  StructField(\"gender\", StringType(), True),\n  StructField(\"salary\", IntegerType(), True)\n])\n \ndf = spark.createDataFrame(data=data2, schema=schema)\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0f620ddc-c4ee-403a-94ef-abfb0eeb3090"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n+---------+----------+---------+-----+------+------+\n|firstname|middlename|lastname |id   |gender|salary|\n+---------+----------+---------+-----+------+------+\n|John     |          |Smith    |36636|M     |2500  |\n|Jane     |          |Doe      |42114|F     |500   |\n|Richard  |Laurence  |Marquette|97086|M     |1500  |\n|Israel   |          |Israeli  |     |M     |3000  |\n|Edward   |III       |         |SL4  |M     |5000  |\n+---------+----------+---------+-----+------+------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- firstname: string (nullable = true)\n-- middlename: string (nullable = true)\n-- lastname: string (nullable = true)\n-- id: string (nullable = true)\n-- gender: string (nullable = true)\n-- salary: integer (nullable = true)\n\n+---------+----------+---------+-----+------+------+\nfirstname|middlename|lastname |id   |gender|salary|\n+---------+----------+---------+-----+------+------+\nJohn     |          |Smith    |36636|M     |2500  |\nJane     |          |Doe      |42114|F     |500   |\nRichard  |Laurence  |Marquette|97086|M     |1500  |\nIsrael   |          |Israeli  |     |M     |3000  |\nEdward   |III       |         |SL4  |M     |5000  |\n+---------+----------+---------+-----+------+------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Create DataFrame from Data sources"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c9e70cfd-c88d-462a-ba02-2ed4b51b8d50"}}},{"cell_type":"markdown","source":["In real-time mostly you create DataFrame from data source files like CSV, Text, JSON, XML e.t.c.\n\nPySpark by default supports many data formats out of the box without importing any libraries and to create DataFrame you need to use the appropriate method available in **DataFrameReader** class.\n\n##### Creating DataFrame from CSV\n\nUse `csv()` method of the `DataFrameReader` object to create a DataFrame from CSV file. you can also provide options like what delimiter to use, whether you have quoted data, date formats, infer schema, and many more.\n\n    df2 = spark.read.csv(\"/src/resources/file.csv\")\n\n##### Creating from text (TXT) file\n\nSimilarly you can also create a DataFrame by reading a from Text file, use `text()` method of the `DataFrameReader` to do so.\n\n    df2 = spark.read.text(\"/src/resources/file.txt\")\n\n##### Creating from JSON file\n\nPySpark is also used to process semi-structured data files like JSON format. you can use `json()` method of the `DataFrameReader` to read JSON file into DataFrame.\n\n    df2 = spark.read.json(\"/src/resources/file.json\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e13a829f-21a4-40e8-a3df-181b8e8ecee3"}}},{"cell_type":"code","source":["v_source_file = \"/FileStore/tables/ufo.csv\"\n\ndf_csv = (\n  spark\n  .read\n  .format('csv')\n  .option('header','true')\n  .option('inferSchema', 'false')  \n  .option('delimiter',',')                                \n  .option('quote', '\\\"')\n  .load(v_source_file)\n)\n\ndf_csv.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a56fc74a-aba9-4551-bdca-33ef889cd467"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+------------+-----------+------------+----------+-----+---------+--------+\n|    LATITUDE|  LONGITUDE|        CITY|DATE_OCCUR|STATE|UFO_SHAPE|DURATION|\n+------------+-----------+------------+----------+-----+---------+--------+\n|-95.78950112|36.05084018|Broken Arrow|11/22/2008|   OK| Changing|       0|\n|-118.2453206|34.05349094| Los Angeles|04/08/2006|   CA| Changing|       0|\n|-106.6486413|35.08418003| Albuquerque|05/26/2007|   NM|   Circle|       0|\n|-71.53660023|43.20725072|     Concord|06/15/2001|   NH|   Circle|       0|\n|-76.51119034|43.45646037|      Oswego|11/10/2005|   NY|   Circle|       0|\n+------------+-----------+------------+----------+-----+---------+--------+\nonly showing top 5 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------+-----------+------------+----------+-----+---------+--------+\n    LATITUDE|  LONGITUDE|        CITY|DATE_OCCUR|STATE|UFO_SHAPE|DURATION|\n+------------+-----------+------------+----------+-----+---------+--------+\n-95.78950112|36.05084018|Broken Arrow|11/22/2008|   OK| Changing|       0|\n-118.2453206|34.05349094| Los Angeles|04/08/2006|   CA| Changing|       0|\n-106.6486413|35.08418003| Albuquerque|05/26/2007|   NM|   Circle|       0|\n-71.53660023|43.20725072|     Concord|06/15/2001|   NH|   Circle|       0|\n-76.51119034|43.45646037|      Oswego|11/10/2005|   NY|   Circle|       0|\n+------------+-----------+------------+----------+-----+---------+--------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### The end of the notebook"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a02c630d-e0e7-4a5a-8a3d-c913ca760394"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"dataframe-create","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3910810181998584}},"nbformat":4,"nbformat_minor":0}
